---
title: 'Wizard Wells: Formatting'
author: "Jill Deines"
date: "April 11, 2018"
output: 
  html_document:
    toc: true
    toc_float: true
    keep_md: true
---

Goal: Load well data from Kansas's WIZARD database, explore dataset, and format for subsequent use.

Update 5/27/2018: removing bad well flagged by Jim Butler

Note: well data is not included in this repo to leave personal data access to the management of the KGS and WIZARD database. The data used here (site and wlevel data) was downloaded for GMD4 on April 11, 2018 from http://www.kgs.ku.edu/Magellan/WaterLevels/index.html

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.path='../figure/00.20_wizardFormatting/',
                      cache = FALSE)
```

**R Packages Needed**

```{r packages, warning=FALSE, message=FALSE}
library(tidyverse) # for ggplot2, tidyr, dplyr
library(sf)
library(lubridate)

# get filepath to R project on local system
library(here)
mainDir <- here()

sessionInfo()
```

# Load Data

## Study Area Boundaries
```{r loadPolys}
gisDir <- paste0(mainDir, '/data/GIS/boundaries')
datum <- 4269 #NAD83 = 4269

# load and re-project to NAD83 (to match WIZARD lat/long)
sheridan <- read_sf(paste0(gisDir,'/Sheridan6_fromKGS_sd6.shp')) %>%
  st_transform(4269)
null9 <- read_sf(paste0(gisDir,'/Sheridan_Null_Geo9.shp')) %>%
  st_transform(4269) %>%
  st_buffer(0)
gmd4 <- read_sf(paste0(gisDir,'/KS_GMD4.kml')) %>%
  st_transform(4269)

# combine study areas
aoi <- sheridan %>%
  mutate(masterid = 'sheridan') %>%
  select(masterid) %>%
  rbind(., null9 %>% select(masterid))

# buffer the study regions and dissolve (to filter wells for kriging)
aoiBuff <- aoi %>% 
  st_transform(5070) %>% 
  st_buffer(10000) %>% 
  st_transform(4269) %>%
  st_union() %>%
  st_sf() 
names(aoiBuff) <- 'geometry'
st_geometry(aoiBuff) <- 'geometry'
aoiBuff$masterid = 'buffer'

# # export for external data processing
# write_sf(aoiBuff, paste0(gisDir,'/Combined_Null9_S6_10kmBuff.shp'))

# punch holes in buffer
buffHole <- st_difference(aoiBuff, sheridan)
buffHoles <- st_difference(buffHole, null9) %>% select('masterid')
plot(buffHoles)

# new buffered aoi polygons
aoi2 <- aoi %>%
  rbind(., buffHoles)
plot(aoi2)

# # export for external data processing
# write_sf(aoi2, paste0(gisDir,'/Combined_Null9_S6_10kmBuff_indvlPolys.shp'))

```

## Well Data
site and wlevel data downloaded for GMD4 on April 11, 2018 
from http://www.kgs.ku.edu/Magellan/WaterLevels/index.html

```{r loadData}
wellDir <- paste0(mainDir, '/data/wellData/WIZARD')
siteFile <- 'raw/sites20180412183715707_GMD4_depthWells.txt'
wlevelFile <- 'raw/wlevel20180412183715707_GMD4_depthWells.txt'

# load site locations
sites <- read_csv(paste0(wellDir,'/',siteFile)) %>%
  select(c(USGS_ID, LATITUDE, LONGITUDE, 
           LAND_SURFACE_ALTITUDE, LAND_SURFACE_ALTITUDE_ACCURACY,
           ALTITUDE_DATUM, USE_OF_SITE_PRIMARY,
           USE_OF_WATER_PRIMARY))

# load water levels
wlevels <- read_csv(paste0(wellDir,'/',wlevelFile)) %>%
  select(c(USGS_ID,  MEASUREMENT_DATE_AND_TIME, DEPTH_TO_WATER, ACCURACY_CODE, STATUS))
```

Note parsing errors aren't a problem

## Subset wells for study area
Hm maybe with a buffer in the future?

```{r wellSubset}
# spatialize well dataset
sitesLL <- st_as_sf(sites, coords = c('LONGITUDE','LATITUDE'), crs = 4269)

# extract points in both aois
wellsIn <- st_join(sitesLL, aoi2) %>% filter(!is.na(masterid))

# # plot
# plot(aoi2, main='water level wells, any year')
# plot(wellsIn[2], add=TRUE, col='black')
```

## Join Water Level Data and Drop Excess
Add water level data to site wells; parse some date columns. Keep only relevant columns

```{r joinWellData}
# add water levels for subsetted wells
wellsInData <- wellsIn %>%
  left_join(wlevels, by = 'USGS_ID')

# make some date columns
wellsInData$Date <- mdy(wellsInData$MEASUREMENT_DATE_AND_TIME)
wellsInData$DOY <- yday(wellsInData$Date)
wellsInData$Year <- year(wellsInData$Date)
```

## Remove well
USGS ID - 392124100364001 per communication with Jim

```{r removeWell}
# check if well is in dataset
392124100364001 %in% wellsInData$USGS_ID

# remove well, reduce columns
wellsInData2 <- wellsInData %>%
  filter(USGS_ID != 392124100364001) %>%
  select(c(USGS_ID, masterid, Date, DOY, Year, DEPTH_TO_WATER,
           LAND_SURFACE_ALTITUDE, STATUS, ACCURACY_CODE))
```


# Process Data

## Time Filters
filter for relevant observation time and years. Filter includes:

* removing measurements outside of winter months (keep December 10 - February 28 measurements)
* adjusting year to better straddle the new year: so if DOY is < 60, decrease year by 1 (in other words, use the same year as the irrigation pumping season)
* starting at 1996 when KGS took over = more robust (so start date = December 2016, since my valid measurements can span the new year)

```{r filterData, fig.width = 9}
# filter days
startDate <- '1996-12-01'
earliestDOY <- 344
latestDOY <- 59

# filter by DOY: keep only winter measurements
wellsWinter <- wellsInData2 %>% 
  filter(DOY <= latestDOY | DOY >= earliestDOY)

# adjust years to indicate year of pumping
wellsWinter$YearAdjusted <- wellsWinter$Year
for (i in 1:nrow(wellsWinter)) {
  if (wellsWinter$DOY[i] < 60) {
    wellsWinter[i,'YearAdjusted'] <- wellsWinter$Year[i] - 1
  }
}

# keep more recent data
length(unique(wellsWinter$USGS_ID))

wellsWinterRecent <- wellsWinter %>% filter(Date >= startDate)

length(unique(wellsWinterRecent$USGS_ID))

# when do recent well winter measurements tend to occur?
ggplot(wellsWinterRecent, aes(DOY)) +
  geom_histogram() +
  facet_wrap(~Year) +
  theme_bw() +
  ggtitle(paste0('Well Reading Date between ', earliestDOY, ' and ', latestDOY))

# compare against all wells
ggplot(wellsInData2 %>% filter(Date >= startDate), aes(DOY)) +
  geom_histogram() +
  geom_vline(xintercept = earliestDOY, col = 'red') +
  geom_vline(xintercept = latestDOY, col= 'red') +
  facet_wrap(~Year) +
  theme_bw() +
  ggtitle(paste0('Well Reading Date between ', earliestDOY, ' and ', latestDOY))

```

## check for bad data
The Status and Accuracy code columns have some qa/qc materials

There are some rows without an entry; the metadata for status says "if blank, the water level was static" so I guess that's a good indication.

There are some Accuracy Code columns with NA's (98) - I remove these (many seem to be multiple measurements of the same well within the target period, perhaps measured with a different system that doens't record accuracy)

```{r qaqc}
# remove rows with NA depths
wells2 <- wellsWinterRecent %>% filter(!is.na(DEPTH_TO_WATER))

# Check accuracy codes
unique(wells2$ACCURACY_CODE)  # 2 is great!
sum(is.na(wells2$ACCURACY_CODE))
wells2b <- wells2 %>% filter(!is.na(ACCURACY_CODE))

# check existing statuses
unique(wells2b$STATUS)
wells2b %>% filter(STATUS == 'A')

# remove 4 anomalous recoreds
wells3_0 <- wells2b %>% filter(is.na(STATUS)) %>%
  select(-c(STATUS))
```

## Collapse Redundant Readings
There are several cases where multiple well levels were recorded for 1 well during each "data season" - currently December 10 - February 28.

Collapse these into 1 value per well per year - using mean until told otherwise.

Note visual inspection of these instances mostly revealed these were measurements recorded on the same day or within a few days with depth values quite similar (off by some hundredths)

Also add a simpler well id column so it's short enough to see when printed to console

```{r removeRedundant}
# add a simpler/shorter wellID
wellKeyUpdater <- data.frame(USGS_ID = unique(wells3_0$USGS_ID),
                             wellID = 1:length(unique(wells3_0$USGS_ID)))

# get mean depth level for each well, in each adjusted year
wells3_1 <- wells3_0 %>%
  left_join(wellKeyUpdater, by = "USGS_ID") %>% 
 # group by wellID and YearAdjusted, add other columns simply to retain values
  group_by(USGS_ID, masterid, wellID, LAND_SURFACE_ALTITUDE, YearAdjusted) %>%
  summarize(Depth_ft = mean(DEPTH_TO_WATER),
            Date = mean(Date), DOY = mean(DOY), Year = mean(Year)) %>%
  arrange(wellID, YearAdjusted)

# add a lat and long column based on sf geometry column
coords <- st_coordinates(wells3_1) 
wells3 <- cbind(wells3_1, coords)
```

## Convert Units 
conversions:

* convert depth-to-water from feet to meters
* convert land surface altitude to meters (Sea Level Datum of 1929) - assumed to be in feet although Metadata doesn't specify
* derive water table elevation

```{r convertUnits}
# feet to meters
wells3$Depth_m <- wells3$Depth_ft * 0.3048 
wells3$SurfaceAltitude_m <- wells3$LAND_SURFACE_ALTITUDE * 0.3048 

# derive water table elevation
wells3$wtElev_m <- wells3$SurfaceAltitude_m - wells3$Depth_m
```


## Calculate Change in Water Level
Include a column for change in water level

Calculated as change since previous year, with negatives indicating a water level drop. Calculated by well. Also flags the number of years since last measurement, so "change in water level" values over more than 1 year could be omitted.

Also add a "better" (shorter) wellID

```{r deltaWater}
# calculate the change in water table elevation since previous measurement, by well
wells4 <- wells3 %>%
  group_by(wellID) %>%
  arrange(wellID,YearAdjusted) %>%
  mutate(changeInWaterLevel = c(NA, diff(wtElev_m))) %>%
  # and add the number of years that "change" spanned
  mutate(changeInterval = c(NA, diff(YearAdjusted))) %>%
  mutate(DOY = as.integer(DOY), 
         changeInterval = as.integer(changeInterval)) %>%
  rename(YearAdj = YearAdjusted,
         dWtElevm = changeInWaterLevel,
         Interval = changeInterval) %>%
  # and select/order columns to retain
  select(c(wellID, masterid, Date, DOY, Year, YearAdj, Depth_m, 
           wtElev_m, dWtElevm, Interval, X, Y))
```

Note I removed the USGS_ID because it caused a "too long" error when writing out the shapefile and I didn't feel like fixing it.

## Summarize Data
Some stats on the remaining data

```{r dataStats}
# how many unique wells?
length(unique(wells4$wellID))

# unique wells by region?
wells4 %>% 
  group_by(masterid) %>%
  summarize(n = n_distinct(wellID))

# how many single-year change data points?
sum(wells4$Interval ==1, na.rm=TRUE)

# how many years by well?
yearCounts <- wells4 %>% 
  group_by(wellID) %>%
  summarize(n = n_distinct(YearAdj))
hist(yearCounts$n)

# how many wells each year?
wellNums <- wells4 %>%
  group_by(YearAdj) %>%
  summarize(n = n())
summary(wellNums$n)
```


# Export Cleaned Data
export for downstream analyses

```{r export, eval=FALSE}
write_sf(wells4, 
         paste0(wellDir,'/cleaned/wells_cleaned_clipped_1996-2017_jimRemoved.shp'))
write_sf(wells4, 
         paste0(wellDir,'/cleaned/wells_cleaned_clipped_1996-2017_jimRemoved.csv'))
```


